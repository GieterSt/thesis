#!/usr/bin/env python3
"""
COMPREHENSIVE LED OPTIMIZATION LLM ANALYSIS SYSTEM (REFACTORED)
Generates complete analysis matching README.md standards including:
- Performance grades and rankings
- Statistical significance testing
- Seasonal performance breakdowns
- Cost-performance analysis
- Automatic README generation
- Automatic HTML generation for publication
- Thesis-ready results
- Centralized model configuration to ensure consistent naming
"""
import json
import os
import time
import glob
import pandas as pd
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from scipy import stats
from scipy.stats import spearmanr, pearsonr, t
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import warnings
import markdown

# --- Centralized Model Configuration ---
MODEL_CONFIG = {
    'deepseek-r1-0528': {
        'display_name': 'DeepSeek R1 (671B)',
        'short_name': 'DeepSeek R1',
        'parameters': 671e9,
        'filename_pattern': 'deepseek-r1-0528'
    },
    'claude-3.7-sonnet': {
        'display_name': 'Claude 3.7 Sonnet (200B)',
        'short_name': 'Claude 3.7',
        'parameters': 200e9,
        'filename_pattern': 'claude-3.7-sonnet'
    },
    'llama-3.3-70b-instruct': {
        'display_name': 'Llama 3.3 70B',
        'short_name': 'Llama 3.3',
        'parameters': 70e9,
        'filename_pattern': 'llama-3.3-70b-instruct'
    },
    'deepseek-r1-distill-qwen-7b': {
        'display_name': 'DeepSeek R1 Distill (7B)',
        'short_name': 'DeepSeek Distill',
        'parameters': 7e9,
        'filename_pattern': 'deepseek-r1-distill-qwen-7b'
    },
    'mistral-7b-instruct': {
        'display_name': 'Mistral 7B Instruct',
        'short_name': 'Mistral 7B',
        'parameters': 7e9,
        'filename_pattern': 'mistral-7b-instruct'
    }
}

def get_model_config_from_path(filepath):
    """Retrieve model configuration from a result filepath."""
    filename = os.path.basename(filepath).lower()
    for config in MODEL_CONFIG.values():
        if config['filename_pattern'] in filename:
            return config
    warnings.warn(f"Could not identify model from filename: {filename}")
    return {}

# Ground truth data paths
GROUND_TRUTH_PATHS = {
    'json': 'data/input-output pairs json/test_ground_truth.json',
}

# Ensure output directories exist
RESULTS_DIRS = {
    'analysis': 'results/analysis',
    'reports': 'results/analysis_reports',
    'figures': 'results/figures',
}

def ensure_directories():
    """Create all required output directories"""
    for dir_path in RESULTS_DIRS.values():
        Path(dir_path).mkdir(parents=True, exist_ok=True)

def load_ground_truth():
    """Load optimal allocations generated by greedy algorithm for comparison"""
    print("\n" + "="*80)
    print("üìä STEP 1: LOADING GROUND TRUTH DATA")
    print("="*80)
    try:
        gt_path = GROUND_TRUTH_PATHS['json']
        if os.path.exists(gt_path):
            print(f"‚úÖ Loading ground truth from: {gt_path}")
            with open(gt_path, 'r', encoding='utf-8') as f:
                ground_truth = json.load(f)
            print(f"üìà Loaded {len(ground_truth)} ground truth scenarios")
            gt_lookup = {}
            for i, scenario in enumerate(ground_truth):
                allocations = {f"hour_{res['hour']}": res['ppfd_allocated'] for res in scenario['output']['hourly_results']}
                gt_lookup[i] = {
                    'date': scenario['input']['date'],
                    'optimal_allocations': allocations,
                    'scenario_complexity': calculate_scenario_complexity(scenario)
                }
            return gt_lookup
    except Exception as e:
        print(f"‚ùå Error loading ground truth: {e}")
        return None

def calculate_scenario_complexity(scenario):
    """Calculate complexity score for scenario"""
    ppfd_requirement = scenario['input']['daily_total_ppfd_requirement']
    date = scenario['input']['date']
    month = int(date.split('-')[1])
    if month in [12, 1, 2]: season = 'Winter'
    elif month in [3, 4, 5]: season = 'Spring'
    elif month in [6, 7, 8]: season = 'Summer'
    else: season = 'Autumn'
    return {'season': season}

def calculate_ground_truth_metrics(model_allocations, ground_truth_scenario):
    """Compare model allocations against optimal greedy algorithm solution"""
    if not ground_truth_scenario or not model_allocations:
        return None
    optimal_allocations = ground_truth_scenario['optimal_allocations']
    hourly_matches = []
    for hour_key, optimal_value in optimal_allocations.items():
        model_value = model_allocations.get(hour_key, 0)
        is_exact_match = abs(model_value - optimal_value) < 0.01
        hourly_matches.append(is_exact_match)
    return {
        'exact_24h_match': sum(hourly_matches) == 24,
        'hourly_match_rate': sum(hourly_matches) / 24 * 100,
        'scenario_complexity': ground_truth_scenario['scenario_complexity']
    }

def assign_performance_grade(hourly_success_rate):
    """Assign performance grade based on hourly success rate criteria"""
    if hourly_success_rate > 85: return "ü•á A (Excellent)"
    if hourly_success_rate > 75: return "ü•à B (Good)"
    if hourly_success_rate > 60: return "ü•â C (Acceptable)"
    if hourly_success_rate > 40: return "üìä D (Poor)"
    return "‚ùå F (Failed)"

def analyze_single_model(filepath, ground_truth):
    """Comprehensive model analysis using the centralized model config."""
    model_config = get_model_config_from_path(filepath)
    if not model_config:
        return None
    model_name = model_config['display_name']
    print(f"\nüî¨ ANALYZING: {model_name}")

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"‚ùå Error loading data for {model_name}: {e}")
        return None

    # Handle Claude's nested data structure
    if 'statistics' in data and 'responses' in data['statistics']:
        results_data = data['statistics']['responses']
    else:
        results_data = data

    total_cases = len(results_data)
    api_successes = sum(1 for d in results_data if d.get('success', False))
    json_valid = sum(1 for d in results_data if d.get('parsed_allocation') is not None)

    ground_truth_comparisons = [calculate_ground_truth_metrics(d.get('parsed_allocation', {}).get('allocation_PPFD_per_hour'), ground_truth.get(i)) for i, d in enumerate(results_data) if d.get('parsed_allocation')]
    valid_comparisons = [c for c in ground_truth_comparisons if c]

    if not valid_comparisons:
        return None

    mean_hourly_match_rate = np.mean([c['hourly_match_rate'] for c in valid_comparisons])
    optimization_success_rate = sum(1 for c in valid_comparisons if c['exact_24h_match']) / total_cases * 100 if total_cases > 0 else 0

    final_metrics = {
        'model_name': model_name,
        'model_config': model_config,
        'basic_performance': {
            'api_success_rate': api_successes / total_cases * 100,
            'json_validity_rate': json_valid / total_cases * 100,
        },
        'ground_truth_analysis': {
            'mean_hourly_match_rate': mean_hourly_match_rate,
            'optimization_success_rate': optimization_success_rate,
        },
        'performance_grade': assign_performance_grade(mean_hourly_match_rate),
        'analysis_by_case': valid_comparisons
    }
    return final_metrics

def comprehensive_statistical_analysis(all_metrics):
    """Performs statistical analysis using centralized model config."""
    print("\n" + "="*80)
    print("üìà STATISTICAL ANALYSIS (REFACTORED)")
    print("="*80)

    regression_data = []
    for metrics in all_metrics.values():
        config = metrics.get('model_config')
        if config and config.get('parameters'):
            regression_data.append({
                'log_params': np.log10(config['parameters']),
                'performance': metrics['ground_truth_analysis']['optimization_success_rate']
            })

    if len(regression_data) < 2: return None
    df = pd.DataFrame(regression_data)
    X = df[['log_params']]
    y = df['performance']
    reg = LinearRegression().fit(X, y)
    r2 = r2_score(y, reg.predict(X))
    print("   ‚úÖ Regression analysis completed.")
    return {'regression': {'r_squared': r2, 'slope': reg.coef_[0], 'intercept': reg.intercept_}}

def create_thesis_visualizations(all_metrics, stats_results, timestamp):
    """Generate all visualizations for the thesis report."""
    print("\n" + "="*80)
    print("üé® CREATING THESIS VISUALIZATIONS (REFACTORED)")
    print("="*80)

    plot_data = []
    for metrics in all_metrics.values():
        config = metrics['model_config']
        plot_data.append({
            'display_name': config['display_name'],
            'short_name': config['short_name'],
            'params': config['parameters'],
            'log_params': np.log10(config['parameters']),
            'performance': metrics['ground_truth_analysis']['optimization_success_rate'],
            'api_success': metrics['basic_performance']['api_success_rate'],
            'json_success': metrics['basic_performance']['json_validity_rate']
        })
    
    df = pd.DataFrame(plot_data).sort_values('params').reset_index(drop=True)

    # --- Visualization 1: Scaling Law Log-Linear Plot ---
    fig, ax = plt.subplots(figsize=(12, 8))
    sns.scatterplot(data=df, x='log_params', y='performance', hue='display_name', s=200, style='display_name', ax=ax, palette='viridis')
    if stats_results and 'regression' in stats_results:
        reg_res = stats_results['regression']
        x_vals = np.array(ax.get_xlim())
        y_vals = reg_res['intercept'] + reg_res['slope'] * x_vals
        ax.plot(x_vals, y_vals, '--', color='red', label=f'Linear Fit (R¬≤={reg_res["r_squared"]:.3f})')
        
    ax.set_title('LLM Performance vs. Model Size (Log-Linear Scaling)', fontsize=16)
    ax.set_xlabel('Model Parameters (log10 scale)', fontsize=12)
    ax.set_ylabel('Optimization Success Rate (%)', fontsize=12)
    ax.legend(title='Models', bbox_to_anchor=(1.05, 1), loc='upper left')
    ax.grid(True, which='both', linestyle='--', linewidth=0.5)
    plt.tight_layout(rect=[0, 0, 0.8, 1])
    path = os.path.join(RESULTS_DIRS['figures'], f'scaling_law_regression_{timestamp}.png')
    plt.savefig(path, dpi=300)
    plt.close()
    print(f"   ‚úÖ Saved Scaling Law plot to {path}")
    
    # --- Add other plots here if needed ---
    return {'scaling_law_plot': path}

def generate_comprehensive_readme(all_metrics, stats_results, visualizations, timestamp):
    """Generates a comprehensive README.md file content."""
    
    # Sort models by performance
    sorted_models = sorted(all_metrics.values(), key=lambda x: x['ground_truth_analysis']['mean_hourly_match_rate'], reverse=True)

    # --- Header ---
    readme = f"# Comprehensive LLM Performance Analysis\n\n"
    readme += f"*Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n\n"
    
    # --- Summary Table ---
    readme += "## üöÄ Overall Performance Summary\n\n"
    readme += "| Model | Grade | Optimization Success | Hourly Match | API Success | JSON Validity |\n"
    readme += "|:---|:---:|:---:|:---:|:---:|:---:|\n"
    for metrics in sorted_models:
        readme += f"| **{metrics['model_name']}** | {metrics['performance_grade']} | {metrics['ground_truth_analysis']['optimization_success_rate']:.1f}% | {metrics['ground_truth_analysis']['mean_hourly_match_rate']:.1f}% | {metrics['basic_performance']['api_success_rate']:.1f}% | {metrics['basic_performance']['json_validity_rate']:.1f}% |\n"
    
    # --- Scaling Law Analysis ---
    readme += "\n##  Scaling Law Analysis\n\n"
    if visualizations and 'scaling_law_plot' in visualizations:
        # Make path relative for markdown file
        figure_path = os.path.relpath(visualizations['scaling_law_plot'], os.path.dirname(RESULTS_DIRS['reports']))
        readme += f"![Scaling Law Plot]({figure_path})\n\n"

    if stats_results and 'regression' in stats_results:
        reg = stats_results['regression']
        readme += f"**Log-Linear Regression Results:**\n"
        readme += f"- **Model**: `Success = {reg['slope']:.2f} * log10(Parameters) + {reg['intercept']:.2f}`\n"
        readme += f"- **R-squared**: `{reg['r_squared']:.3f}`\n"

    return readme

def generate_html_from_readme(readme_content, timestamp):
    """Converts README markdown to a styled HTML file."""
    print("\n" + "="*80)
    print("üìÑ GENERATING HTML REPORT")
    print("="*80)
    
    html_template = """
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>LLM Performance Analysis</title>
        <style>
            body {{ font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif; line-height: 1.6; color: #333; max-width: 900px; margin: 20px auto; padding: 20px; }}
            h1, h2, h3 {{ border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }}
            img {{ max-width: 100%; height: auto; display: block; margin: 20px 0; border: 1px solid #ddd; border-radius: 4px; padding: 5px; }}
            table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
            th, td {{ border: 1px solid #dfe2e5; padding: 8px 12px; text-align: left; }}
            th {{ background-color: #f6f8fa; font-weight: bold; }}
            tr:nth-child(even) {{ background-color: #f6f8fa; }}
            code {{ background-color: #f1f1f1; padding: 0.2em 0.4em; margin: 0; font-size: 85%; border-radius: 3px; }}
        </style>
    </head>
    <body>
        {content}
    </body>
    </html>
    """
    
    html_content = markdown.markdown(readme_content, extensions=['markdown.extensions.tables'])
    
    final_html = html_template.format(content=html_content)
    
    html_path = os.path.join(RESULTS_DIRS['reports'], f'LLM_Analysis_Report_{timestamp}.html')
    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(final_html)
        
    print(f"   ‚úÖ Saved HTML report to {html_path}")
    return html_path

def main():
    """Main execution function."""
    ensure_directories()
    ground_truth = load_ground_truth()
    if not ground_truth:
        print("‚ùå Could not load ground truth data. Aborting.")
        return

    all_metrics = {}
    result_files = glob.glob('results/model_outputs/*.json')
    if not result_files:
        print("üìÇ No result files found in 'results/model_outputs/'.")
        return

    for f in result_files:
        metrics = analyze_single_model(f, ground_truth)
        if metrics:
            all_metrics[metrics['model_name']] = metrics
            
    if not all_metrics:
        print("ü§∑ No models were analyzed successfully.")
        return
        
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    stats_results = comprehensive_statistical_analysis(all_metrics)
    visualizations = create_thesis_visualizations(all_metrics, stats_results, timestamp)
    
    readme_content = generate_comprehensive_readme(all_metrics, stats_results, visualizations, timestamp)
    html_path = generate_html_from_readme(readme_content, timestamp)
    
    print("\n\n‚úÖ Refactored analysis script finished successfully!")
    print(f"Figures with corrected model names have been generated in the '{RESULTS_DIRS['figures']}' directory.")
    print(f"The final HTML report is available at: {html_path}")

if __name__ == '__main__':
    main()